\section{Metaheurística GRASP}
	\subsection{Desarrollo}
Hemos propuesto dos métodos que computan una solución, formulándola en base a criterios heurísticos. La limitación de los enfoques anteriores reside en que se recorre el espacio de soluciones hasta que no es posible mejorar la solución. Como sabemos que una solución maximal no necesariamente es máxima, sería útil poder contrastar distintas soluciones maximales y elegir a la mejor. En otras palabras quisiéramos ramificar la exploración del espacio de soluciones.

Habiendo notado que heurísticas determinísticas siempre toman la misma decisión en el mismo paso, se propone la utilización de una heurística pseudo-greedy. Esta heurística aleatoriamente tomará decisiones localmente buenas pero no necesariamente óptimas. Se considerará que la decisión de agregar un nodo es buena si agregarlo aumenta el tamaño de la frontera. Para tener cierto control sobre que tan goloso es el comportamiento de esta heurística se puede pedir que la elección aleatoria sea tomada teniendo en cuenta solo el mejor porcentaje de las decisiones posibles. Si el porcentaje es chico, solo se consideraran las mejores opciones y el comportamiento será muy similar a la heurística greedy pura. Esto restringiría la ramificación que buscabamos. Si el porcentaje es muy alto existirá la posibilidad de agregar un nodo de grado muy bajo a la clique, lo cual restringirá en gran medida la cantidad de nodos que se pueden agregar, obteniendo muy posiblemente una clique pequeña. 


\begin{algorithm}[H]
	\NoCaptionOfAlgo
	\caption{\algoritmo{randomGreedy}{\In{listaAdyacencia}{lista}, \In{float}{porcentajeConsiderado}}{clique}}
	
	nodosConsiderados $\leftarrow$ nodos(lista)
	
	ordenarPorGrado(nodosConsiderados)
	
	indiceNodoAleatorio $\leftarrow$ nodoAleatorio(nodosConsiderados, porcentajeConsiderado)
	
	nodoPorAgregar $\leftarrow$ nodosConsiderados[indiceNodoAleatorio]
	
	agregarNodoAClique(res, nodoPorAgregar)
	
	nodosConsiderados $\leftarrow$ nodoPorAgregar.adyacentes()

	ordenarPorGrado(nodosConsiderados)
	
	res $\leftarrow$ recurRandomGreedy(lista, res, nodosConsiderados, porcentajeConsiderado)


\end{algorithm}

\begin{algorithm}[H]
	\NoCaptionOfAlgo
	\caption{\algoritmo{recurRandomGreedy}{\In{listaAdyacencia}{lista}, \In{clique}{cliqueParcial}, \In{listaNodos}{nodosConsiderados}\In{float}{porcentajeConsiderado}}{clique}}
	
	\If{nodosConsiderados.size() $=$ 0}
	{
		return cliqueParcial
	}
	
	\For{nodo $\in$ nodosConsiderados}
	{
		\If{nodo.grado() $<$ cliqueParcial.size() * 2 $\vee$ nodo.esAdyacenteATodos(clique, lista)}
		{
			nodosConsiderados.borrar(nodo)
		}
	}

	\If{nodosConsiderados.size() $=$ 0}
	{
		return cliqueParcial
	}
	
	indiceNodoAleatorio $\leftarrow$ nodoAleatorio(nodosConsiderados, porcentajeConsiderado)
	
	nodoPorAgregar $\leftarrow$ nodosConsiderados[indiceNodoAleatorio]
	
	agregarNodoAClique(res, nodoPorAgregar)

	nodosConsiderados.borrar(nodoPorAgregar)
	
	res $\leftarrow$ recurRandomGreedy(lista, cliqueParcial, nodosConsiderados, porcentajeConsiderado)

\end{algorithm}

\begin{algorithm}[H]
	\NoCaptionOfAlgo
	\caption{\algoritmo{nodoAleatorio}{\In{listaNodos}{nodosConsiderados}\In{float}{porcentajeConsiderado}}{clique}}
	
		cantidadPorConsiderar $\leftarrow$ nodosConsiderados.size() $*$ (1 - porcentajeConsiderado)
		
		res $\leftarrow$ random(rango(cantidadPorConsiderar))

\end{algorithm}

Cada vez que se elige un nodo, se hace eligiendo un nodo aleatorio que esté entre los mejores de la lista de nodos agregables. En un principio, todos los nodos son elegibles para formar una clique trivial de tamaño uno. El criterio utilizado para elegir alguno es la priorización de nodos de grado alto. Es por esto que en primer lugar se ordenan los nodos en base a su grado. Después se elige un nodo aleatorio entre los de mayor grado. El porcentaje a considerar es una variable de entrada que determinará el comportamiento de la heurística.

La función recursiva tiene un procesamiento muy similar, pero toma como parámetro a una clique y una lista de nodos a considerar. En el caso base, si no quedan nodos por considerar, la clique es maximal. Sino, toma la lista y le filtra los nodos que no son adyacentes a todos los nodos de la clique o que no agrandarían la frontera por tener un grado muy chico. Vuelve a preguntar si quedan nodos a considerar y en caso afirmativo elige un nodo aleatorio entre los mejores y lo agrega a la clique. Elimina a ese nodo de la lista de nodos a considerar y se llama recursivamente. Como en cada paso la cantidad de nodos a considerar disminuye al menos en una unidad, sabemos que la función eventualmente llega al caso base.

La metaheurística GRASP utiliza tanto búsqueda local como greedy aleatorio. La idea esta en que greedy aleatorio avanza estocásticamente por el espacio de soluciones hasta que llega a una solución maximal. Posteriormente esta solución se pasa como parametro a la búsqueda local. Si hacemos esto muchas veces tenemos la posibilidad de llegar a muchas soluciones diferentes y así quedarnos con la mejor. Se memoriza la mejor encontrada y en cada iteración del ciclo se compara una nueva solución. Si iteramos lo suficiente, tendremos seguridad de que la solución que guardamos es la mejor entre muchas posibilidades. 

\begin{algorithm}[H]
	\NoCaptionOfAlgo
	\caption{\algoritmo{grasp}{\In{listaAdyacencia}{lista}, \In{unsigned int}{iteraciones}, \In{float}{porcentajeConsiderado}}{clique}}

	bestClique $\leftarrow \emptyset$
	 
	\For{i $\in$ rango(iteraciones)}
	{
		tempClique $\leftarrow$ local(randomGreedy(lista, porcentajeConsiderado))
		 
 		\If{bestClique.frontera() $<$ tempClique.frontera()}
		{
			bestClique $\leftarrow$ tempClique
		}
		 
	}
	
	res $\leftarrow$ bestClique

\end{algorithm}

\subsection{Cota temporal}
La complejidad de randomGreedy está dada por:

\begin{itemize}
    \item \textbf{•}xtbf{nodos(lista)}: Devuelve una lista que contiene a todos los nodos del grafo en $O(n)$.

	\item \textbf{ordenarPorGrado}: Esta función utiliza por detrás el sort de la STD, y por ende su complejidad en peor caso es de $O(n*log(n))$.

	\item \textbf{indiceNodoAleatorio}: Devuelve un número aleatorio en $O(1)$.

	\item \textbf{agregarNodoAClique}: Se encarga de actualizar la clique agregando atrás del vector de nodos en la clique el nodo a insertar en $O(1)$.	

	\item \textbf{agregarNodoAClique}: Se encarga de actualizar la clique agregando atrás del vector de nodos en la clique el nodo a insertar en $O(1)$.	

\end{itemize}

Vemos que el costo de esta función es dependerá del costo de la función recursiva:

\begin{itemize}

    \item \textbf{esAdyacenteATodos}: Busca si hay un nodo en la clique que no sea adyacente a este nuevo nodo. Para eso, arma un arreglo de booleanos y recorre los nodos adyacentes al nodo a insertar (pueden ser hasta $n-1$). Después, basta con recorrer los nodos de la clique y fijarse en el arreglo si son adyacentes o no. Siguiento este procedimiento, el algoritmo tiene una complejidad temporal $O(|adyacentes| + |clique|)$, donde $O(|adyacentes|)$ $\subseteq$ $O(n)$ y $O(|clique|)$ $\subseteq$ $O(n)$. Por lo tanto, tenemos $O(n + n)$  $\subseteq$ $O(n)$.

    \item \textbf{agregarNodoAClique}: Se encarga de actualizar la clique agregando atrás del vector de nodos en la clique el nodo a insertar en $O(1)$.

\end{itemize}

Dado que la función recursiva termina cuando el parámetro nodosPorConsiderar es de tamaño cero y que en el peor de los casos puede empezar siendo de tamaño $O(n)$ y decrecer en una unidad en cada llamada recursiva, se concluye que en el peor de los casos se realizarán $O(n)$ llamadas recursivas. En cada una hay un ciclo de $O(nodosPorConsiderar)$ iteraciones y adentro se llama a la funcion esAdyacenteATodos que es $O(n)$ como en el peor de los casos nodosPorConsiderar decrece de a una unidad, la complejidad será $O(n * \Sigma in) = O(n^2 * \Sigma i) = O(n^3)$.

Dado que GRASP esta compuesto por un ciclo que corre tantas veces como se le especifique en el parámetro iteraciones, el costo temporal va a depender linealmente del número de iteraciones. En cada iteración del ciclo se realiza una llamada a localSearch(randomGreedy()) lo cual cuesta $O(n^3 + n^6) = O(n^6)$. Por lo tanto el costo de la metaheuristica GRASP será $O(iteraciones * n^6)$


\subsection{Experimentación}

Al experimentar con la metaheurística, primero decidimos analizar el impacto lineal de las iteraciones:

\noindent
\begin{minipage}{0.55\textwidth}
    \hfill
    \includegraphics[scale=0.6]{img/grasp-it.png}
\end{minipage}
\hfill
\begin{minipage}{0.44\textwidth}
    \begin{center}
        Datos del gráfico

        \begin{tabular}{ | l l |}
            \hline
             & $n = 10$ \\ 
             & $m = 20$ \\ 
            Porcentaje de nodos & \\
            considerados & $p = 0.5$ \\ 
            Curva aproximada & $f(x) = 72500 * x * 10000$ \\
            \hline
        \end{tabular}
    \end{center}
\end{minipage}

Este resultado era más que esperado, ya que resulta de ejecutar las mismas operaciones una cantidad fija de veces. Teniendo esto en cuenta, podemos analizar los demás factores en el caso de una única iteración.

\noindent
\begin{minipage}{0.55\textwidth}
    \hfill
    \includegraphics[scale=0.6]{img/grasp-p.png}
\end{minipage}
\hfill
\begin{minipage}{0.44\textwidth}
    \begin{center}
        Datos del gráfico

        \begin{tabular}{ | l l |}
            \hline
             & $n = 10$ \\ 
             & $m = 20$ \\ 
            Iteraciones & $it = 1$ \\
            \hline
        \end{tabular}
    \end{center}
\end{minipage}

Por el otro lado, el porcentaje de nodos del grafo a considerar se comporta de manera muy peculiar con respecto a la complejidad. Procedimos a analizar más casos para comprender este comportamiento:

\begin{center}
    \includegraphics[scale=0.6]{img/grasp-p-multi.png}
\end{center}

La variación en performance se debe en particular a 2 motivos. Por un lado, las búsquedas golosas aleatorias generan una cierta variabilidad en los resultados obtenidos, por lo que resulta difícil determinar qué corresponde a ruido y qué a un camino de decisiones distinto. Por el otro, a medida que aumentamos el porcentaje de los nodos a considerar, permitimos que ciertas instancias ``menos óptimas'' (desde una perspectiva golosa) sean utilizadas, lo cual puede llevar a decisiones no tan útiles y, por ende, cliques más pequeñas. Esto aplica en particular a los grafos con menor cantidad de aristas, ya que mientras más tenga, mayor será en general el grado los nodos, y por consiguiente siempre se considerará agregar más nodos a la clique final.

Además de experimentar con los tiempos de ejecución, decidimos analizar la influencia de ambos parámetros de entrada de la metaheurística en la precisión de los resultados obtenidos. Para esto, utilizamos un conjunto de tests de tamaño moderado, cuyas soluciones fueron obtenidas a través del algoritmo exacto. Para cada caso de prueba, probamos distintos valores para dichos parámetros y comparamos el resultado con el obtenido por fuerza bruta.

A fines prácticos, llamaremos $it$ al número de iteraciones realizadas, y $p$ al porcentaje de nodos a considerar (entre 0 y 1).

\begin{center}
    \includegraphics[scale=0.6]{img/grasp-4x4.png}
\end{center}

Este set de resultados nos resultó muy interesante. Pudimos comprobar que, casi siempre, realizar más iteraciones del algoritmo resulta en un resultado mejor (cosa que nos resultaba trivial, aunque podría no haber mejorado mucho). Por otro lado, también confirmamos que, por si solo, considerar más nodos no mejora de manera muy significativa los resultados. Sin embargo, nos tomó por sorpresa que, al realizar múltiples iteraciones ($it \geq 30$), considerar más nodos ($p \geq 0.6$) aparenta también mejorar bastante los resultados, aunque no en todas las situaciones.
 
Se debe tener en cuenta que estos resultados no son 100\% determinísticos, ya que los nodos escogidos varían de acuerdo al valor de p. Sin embargo, en base a este gráfico podemos afirmar que (para los casos de prueba utilizados) realizar aproximadamente 50 iteraciones y considerando el 60\% de los nodos de mayor grado, nuestra implementación de la metaheurística GRASP es casi tan precisa como una búsqueda por fuerza bruta.

\begin{center}
    \includegraphics[scale=0.6]{img/grasp-it-v-p.png}
\end{center}

Al ver más de cerca los datos, promediando las diferencias de todos nuestros casos de prueba, podemos observar la variabilidad de nuestro algoritmo: en promedio, nuestras pruebas dieron resultados igual de precisos con $p = 0.6$ realizando 35 o 50 iteraciones. Es más, para ningún valor de p obtuvimos una linea estrictamente decreciente, que sería lo esperable de algorítmos determinísticos.

También podemos ver que en varias ocasiones, para distintas cantidades de iteraciones, otros valores de p dieron resultados más precisos. Sin embargo, consideramos que aumentar la cantidad de iteraciones reduce la variabilidad introducida por las búsquedas golosas aleatorias (porque al iterar siempre conservamos el mejor resultado), por lo que la mayor cantidad de iteraciones es más representativa del valor óptimo de p.